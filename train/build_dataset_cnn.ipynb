{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rediahmds/eco-sort/blob/main/train/build_dataset_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive"
      ],
      "metadata": {
        "id": "wUCMQEaG8uyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4m-QmN58pFP",
        "outputId": "b410bc5c-f052-4c7d-8230-8b11687d7885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "vXt0alxT9gYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset from Kaggle\n",
        "\n",
        "The dataset comes from different accounts on Kaggle."
      ],
      "metadata": {
        "id": "lpWGiBrz9k8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rRTs29pn92pf",
        "outputId": "298fc7c5-f06e-44f2-9ed3-9f91462899e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "alistair_ds = kagglehub.dataset_download(\"alistairking/recyclable-and-household-waste-classification\")\n",
        "print(\"Path to dataset files:\", alistair_ds)\n",
        "\n",
        "mostafa_ds = kagglehub.dataset_download(\"mostafaabla/garbage-classification\")\n",
        "print(\"Path to dataset files:\", mostafa_ds)\n",
        "\n",
        "joe_ds = kagglehub.dataset_download(\"joebeachcapital/realwaste\")\n",
        "print(\"Path to dataset files:\", joe_ds)\n",
        "\n",
        "glhdamar_ds = kagglehub.dataset_download(\"glhdamar/new-trash-classfication-dataset\")\n",
        "print(\"Path to dataset files:\", glhdamar_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He2r8yLn-Iri",
        "outputId": "288ad9d1-ddbb-437b-a37b-2633af65fa2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/recyclable-and-household-waste-classification\n",
            "Path to dataset files: /kaggle/input/garbage-classification\n",
            "Path to dataset files: /kaggle/input/realwaste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Directory Tree"
      ],
      "metadata": {
        "id": "v73zVPi6QCiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def print_directory_tree(root: Path, prefix: str = \"\"):\n",
        "    \"\"\"\n",
        "    Mencetak struktur direktori dengan tampilan seperti pohon.\n",
        "    Hanya menampilkan folder (tanpa file).\n",
        "    \"\"\"\n",
        "    subdirs = sorted([p for p in root.iterdir() if p.is_dir()])\n",
        "    for i, subdir in enumerate(subdirs):\n",
        "        connector = \"‚îî‚îÄ‚îÄ \" if i == len(subdirs) - 1 else \"‚îú‚îÄ‚îÄ \"\n",
        "        print(f\"{prefix}{connector}{subdir.name}\")\n",
        "        extension = \"    \" if i == len(subdirs) - 1 else \"‚îÇ   \"\n",
        "        print_directory_tree(subdir, prefix + extension)"
      ],
      "metadata": {
        "id": "5LRPepWTP80D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path ke folder utama\n",
        "alistair_path = Path(alistair_ds) / \"images\" / \"images\"\n",
        "mostafa_path = Path(mostafa_ds) / \"garbage_classification\"\n",
        "joe_path = Path(joe_ds) / \"realwaste-main\" / \"RealWaste\"\n",
        "glhdamar_path = Path(glhdamar_ds) / \"new-dataset-trash-type-v2\"\n",
        "\n",
        "print(alistair_path.name)\n",
        "print_directory_tree(alistair_path)\n",
        "\n",
        "print(mostafa_path.name)\n",
        "print_directory_tree(mostafa_path)\n",
        "\n",
        "print(joe_path.name)\n",
        "print_directory_tree(joe_path)\n",
        "\n",
        "print(glhdamar_path.name)\n",
        "print_directory_tree(glhdamar_path)"
      ],
      "metadata": {
        "id": "w02xflVHQIQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restructure AlistairKing Dataset"
      ],
      "metadata": {
        "id": "pLspApePQ0Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_root = alistair_path\n",
        "target_root = Path(\"dataset/train\")\n",
        "target_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class_map = {\n",
        "    \"food_waste\": \"organic\",\n",
        "    \"eggshells\": \"organic\",\n",
        "    \"coffee_grounds\": \"organic\",\n",
        "    \"tea_bags\": \"organic\",\n",
        "    \"plastic_soda_bottles\": \"plastic\",\n",
        "    \"plastic_trash_bags\": \"plastic\",\n",
        "    \"plastic_food_containers\": \"plastic\",\n",
        "    \"plastic_shopping_bags\": \"plastic\",\n",
        "    \"plastic_straws\": \"plastic\",\n",
        "    \"plastic_water_bottles\": \"plastic\",\n",
        "    \"plastic_detergent_bottles\": \"plastic\",\n",
        "    \"plastic_cup_lids\": \"plastic\",\n",
        "    \"glass_food_jars\": \"glass\",\n",
        "    \"glass_beverage_bottles\": \"glass\",\n",
        "    \"glass_cosmetic_containers\": \"glass\",\n",
        "    \"aluminum_soda_cans\": \"metal\",\n",
        "    \"aluminum_food_cans\": \"metal\",\n",
        "    \"steel_food_cans\": \"metal\",\n",
        "    \"aerosol_cans\": \"metal\",\n",
        "    \"cardboard_boxes\": \"paper\",\n",
        "    \"cardboard_packaging\": \"paper\",\n",
        "    \"magazines\": \"paper\",\n",
        "    \"newspaper\": \"paper\",\n",
        "    \"office_paper\": \"paper\",\n",
        "    \"paper_cups\": \"paper\",\n",
        "    \"styrofoam_cups\": \"styrofoam\",\n",
        "    \"styrofoam_food_containers\": \"styrofoam\",\n",
        "    \"clothing\": \"textiles\",\n",
        "    \"shoes\": \"textiles\"\n",
        "}\n",
        "\n",
        "\n",
        "print(\"üöÄ Memulai pengelompokan dataset dengan penamaan ulang...\\n\")\n",
        "\n",
        "for class_name, parent_class in class_map.items():\n",
        "    for subset in [\"default\", \"real_world\"]:\n",
        "        class_dir = source_root / class_name / subset\n",
        "        if class_dir.exists():\n",
        "            img_list = list(class_dir.glob(\"*.*\"))\n",
        "            print(f\"üìÅ Menyalin {len(img_list)} gambar dari '{class_name}/{subset}' ke '{parent_class}'\")\n",
        "            for i, img in enumerate(tqdm(img_list, desc=f\"{class_name}/{subset}\", leave=False)):\n",
        "                dest_dir = target_root / parent_class\n",
        "                dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Format nama: subset_class_####__asli.ext\n",
        "                ext = img.suffix\n",
        "                original_name = img.stem.replace(\" \", \"_\")\n",
        "                new_name = f\"{subset}_{class_name}_{i:04d}__{original_name}{ext}\"\n",
        "                shutil.copy(img, dest_dir / new_name)\n",
        "\n",
        "print(\"\\n‚úÖ Pengelompokan selesai tanpa konflik penamaan.\")\n",
        "print(\"üìÇ Dataset tersimpan di:\", target_root.resolve())\n"
      ],
      "metadata": {
        "id": "bdcyeFneQ5V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge All Dataset\n",
        "\n",
        "Copy from different sources."
      ],
      "metadata": {
        "id": "idyjsruwRNCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def copy_n_files(src_dir, dst_dir, n, randomize=False):\n",
        "    src_path = Path(src_dir)\n",
        "    dst_path = Path(dst_dir)\n",
        "\n",
        "    # Buat folder tujuan jika belum ada\n",
        "    dst_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Ambil semua file dari direktori sumber\n",
        "    all_files = [f for f in src_path.iterdir() if f.is_file()]\n",
        "\n",
        "    # Pastikan n tidak lebih besar dari jumlah file\n",
        "    n = min(n, len(all_files))\n",
        "\n",
        "    # Tentukan file mana yang akan disalin\n",
        "    if randomize:\n",
        "        files_to_copy = random.sample(all_files, n)\n",
        "    else:\n",
        "        files_to_copy = sorted(all_files)[:n]\n",
        "\n",
        "    # Copy file satu per satu\n",
        "    for file in files_to_copy:\n",
        "        shutil.copy(file, dst_path)\n",
        "        print(f\"Copied: {file.name}\")\n",
        "\n",
        "    print(f\"\\nTotal {n} files copied from '{src_dir}' to '{dst_dir}' (random: {randomize}).\")"
      ],
      "metadata": {
        "id": "DrXoFBSeRb1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment all for first run\n",
        "\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/paper\", \"dataset/train/paper\", 500, randomize=True)\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/cardboard\", \"dataset/train/paper\", 500, randomize=True)\n",
        "\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/white-glass\", \"dataset/train/glass\", 600, randomize=True)\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/brown-glass\", \"dataset/train/glass\", 600, randomize=True)\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/green-glass\", \"dataset/train/glass\", 600, randomize=True)\n",
        "\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/clothes\", \"dataset/train/textiles\", 1500, randomize=True)\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/shoes\", \"dataset/train/textiles\", 1500, randomize=True)\n",
        "\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/metal\", \"dataset/train/metal\", 750, randomize=True)\n",
        "copy_n_files(f\"{joe_ds}/realwaste-main/RealWaste/Metal\", \"dataset/train/metal\", 750, randomize=True)\n",
        "\n",
        "copy_n_files(f\"{mostafa_ds}/garbage_classification/biological\", \"dataset/train/organic\", 980, randomize=True)\n",
        "copy_n_files(f\"{glhdamar_ds}/new-dataset-trash-type-v2/organic\", \"dataset/train/organic\", 960, randomize=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1s9htvNPRe4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for Duplicates"
      ],
      "metadata": {
        "id": "nR5mnzbQR6a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper: Auto Reconnect"
      ],
      "metadata": {
        "id": "IoeGRR-oRD0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Time Out Preventer (Advanced)\n",
        "%%capture\n",
        "AUTO_RECONNECT = True #@param {type:\"boolean\"}\n",
        "#@markdown **Run this code to prevent Google Colab from Timeout**\n",
        "from os import makedirs\n",
        "makedirs(\"/root/.config/rclone\", exist_ok = True)\n",
        "if AUTO_RECONNECT:\n",
        "  import IPython\n",
        "  from google.colab import output\n",
        "\n",
        "  display(IPython.display.Javascript('''\n",
        "  function ClickConnect(){\n",
        "    btn = document.querySelector(\"colab-connect-button\")\n",
        "    if (btn != null){\n",
        "      console.log(\"Click colab-connect-button\");\n",
        "      btn.click()\n",
        "      }\n",
        "\n",
        "    btn = document.getElementById('ok')\n",
        "    if (btn != null){\n",
        "      console.log(\"Click reconnect\");\n",
        "      btn.click()\n",
        "      }\n",
        "    }\n",
        "\n",
        "  setInterval(ClickConnect,60000)\n",
        "  '''))"
      ],
      "metadata": {
        "id": "rbmU6CfJRIrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagehash"
      ],
      "metadata": {
        "id": "5Ni9QLRoR_Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_resnet_embedding(img: Image.Image, model, transform, device):\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = model(img_tensor).squeeze()\n",
        "    return features.cpu()\n",
        "\n",
        "\n",
        "def find_duplicates_images_with_end_deletion(\n",
        "    dataset_dir: Path | str,\n",
        "    hash_threshold: int = 5,\n",
        "    sim_threshold: float = 0.98,\n",
        "    delete: bool = False,\n",
        "    show_preview: bool = True\n",
        "):\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    print(f\"üîç Mendeteksi duplikat di dalam: {dataset_dir.resolve()}\")\n",
        "\n",
        "    # === Inisialisasi hash & CNN\n",
        "    hash_dict = defaultdict(list)\n",
        "    duplicates = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cnn_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    cnn_model = torch.nn.Sequential(*list(cnn_model.children())[:-1])\n",
        "    cnn_model.eval().to(device)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for img_path in dataset_dir.rglob(\"*.*\"):\n",
        "        if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            continue\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            h = imagehash.average_hash(img)\n",
        "\n",
        "            # üîÅ Bandingkan dengan semua hash yang mirip\n",
        "            for existing_hash in hash_dict:\n",
        "                if h - existing_hash <= hash_threshold:\n",
        "                    for existing_path in hash_dict[existing_hash]:\n",
        "                        try:\n",
        "                            img1 = Image.open(existing_path).convert(\"RGB\")\n",
        "                            emb1 = get_resnet_embedding(img1, cnn_model, transform, device)\n",
        "                            emb2 = get_resnet_embedding(img, cnn_model, transform, device)\n",
        "\n",
        "                            sim = F.cosine_similarity(emb1, emb2, dim=0).item()\n",
        "                            if sim >= sim_threshold:\n",
        "                                counter += 1\n",
        "                                print(f\"üî¢ Kode duplikat: {counter}\")\n",
        "                                print(f\"‚ö†Ô∏è Duplikat terdeteksi:\")\n",
        "                                print(f\"  Original : {existing_path.name}\")\n",
        "                                print(f\"  Duplicate: {img_path.name}\")\n",
        "                                print(f\"  üîó Cosine Similarity: {sim:.4f}\")\n",
        "\n",
        "                                duplicates.append(img_path)\n",
        "\n",
        "                                if show_preview:\n",
        "                                    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "                                    ax[0].imshow(img1)\n",
        "                                    ax[0].set_title(f\"Original: {existing_path.name}\")\n",
        "                                    ax[1].imshow(img)\n",
        "                                    ax[1].set_title(f\"Duplicate: {img_path.name}\")\n",
        "                                    for a in ax:\n",
        "                                        a.axis(\"off\")\n",
        "                                    plt.tight_layout()\n",
        "                                    plt.show()\n",
        "\n",
        "                                print(\"\\n\")\n",
        "\n",
        "                                break  # cukup validasi 1 yang cocok\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ùå Error validasi CNN: {e}\")\n",
        "                    else:\n",
        "                        continue\n",
        "                    break  # keluar dari hash loop jika sudah match\n",
        "\n",
        "            hash_dict[h].append(img_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Gagal membuka {img_path}: {e}\")\n",
        "\n",
        "    # === Hapus jika diminta\n",
        "    if delete:\n",
        "        for dup in duplicates:\n",
        "            try:\n",
        "                dup.unlink()\n",
        "                print(f\"üóëÔ∏è Menghapus: {dup}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Gagal menghapus {dup}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Total duplikat terverifikasi: {len(duplicates)}\")\n",
        "    return duplicates\n",
        "\n",
        "\n",
        "def find_duplicates_images_with_immediate_deletion(\n",
        "    dataset_dir: Path | str,\n",
        "    hash_threshold: int = 5,\n",
        "    sim_threshold: float = 0.98,\n",
        "    delete: bool = False,\n",
        "    show_preview: bool = True\n",
        "):\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    print(f\"üîç Mendeteksi duplikat di dalam: {dataset_dir.resolve()}\")\n",
        "\n",
        "    hash_dict = defaultdict(list)\n",
        "    duplicates = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cnn_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    cnn_model = torch.nn.Sequential(*list(cnn_model.children())[:-1])\n",
        "    cnn_model.eval().to(device)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for img_path in dataset_dir.rglob(\"*.*\"):\n",
        "        if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            continue\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            h = imagehash.average_hash(img)\n",
        "\n",
        "            for existing_hash in hash_dict:\n",
        "                if h - existing_hash <= hash_threshold:\n",
        "                    for existing_path in hash_dict[existing_hash]:\n",
        "                        try:\n",
        "                            img1 = Image.open(existing_path).convert(\"RGB\")\n",
        "                            emb1 = get_resnet_embedding(img1, cnn_model, transform, device)\n",
        "                            emb2 = get_resnet_embedding(img, cnn_model, transform, device)\n",
        "\n",
        "                            sim = F.cosine_similarity(emb1, emb2, dim=0).item()\n",
        "                            if sim >= sim_threshold:\n",
        "                                counter += 1\n",
        "                                print(f\"üî¢ Kode duplikat: {counter}\")\n",
        "                                print(f\"‚ö†Ô∏è Duplikat terdeteksi:\")\n",
        "                                print(f\"  Original : {existing_path.name}\")\n",
        "                                print(f\"  Duplicate: {img_path.name}\")\n",
        "                                print(f\"  üîó Cosine Similarity: {sim:.4f}\")\n",
        "\n",
        "                                if show_preview:\n",
        "                                    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "                                    ax[0].imshow(img1)\n",
        "                                    ax[0].set_title(f\"Original: {existing_path.name}\")\n",
        "                                    ax[1].imshow(img)\n",
        "                                    ax[1].set_title(f\"Duplicate: {img_path.name}\")\n",
        "                                    for a in ax:\n",
        "                                        a.axis(\"off\")\n",
        "                                    plt.tight_layout()\n",
        "                                    plt.show()\n",
        "\n",
        "                                if delete:\n",
        "                                    try:\n",
        "                                        img_path.unlink()\n",
        "                                        print(f\"üóëÔ∏è Duplikat dihapus: {img_path}\")\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"‚ùå Gagal menghapus {img_path}: {e}\")\n",
        "                                else:\n",
        "                                    duplicates.append(img_path)\n",
        "\n",
        "                                print(\"\\n\")\n",
        "                                break  # match pertama cukup\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ùå Error validasi CNN: {e}\")\n",
        "                    else:\n",
        "                        continue\n",
        "                    break  # hash cocok, stop\n",
        "\n",
        "            hash_dict[h].append(img_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Gagal membuka {img_path}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Total duplikat terverifikasi: {counter}\")\n",
        "    return duplicates"
      ],
      "metadata": {
        "id": "HfVEKSc7SGTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_duplicates_images_with_end_deletion(\n",
        "    \"dataset/train\",\n",
        "    hash_threshold=6,\n",
        "    delete=True,\n",
        "    show_preview=False,\n",
        ")"
      ],
      "metadata": {
        "id": "1nd4lBq5S3wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Dataset Distribution"
      ],
      "metadata": {
        "id": "M_30__aRPhay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_dataset = ImageFolder(\"dataset/train\")\n",
        "label_counts = Counter([label for _, label in train_dataset])\n",
        "print(\"Label mapping:\", train_dataset.class_to_idx)\n",
        "print(\"Distribusi kelas:\", label_counts)"
      ],
      "metadata": {
        "id": "2mjMGELePYkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive Dataset and Save to Google Drive"
      ],
      "metadata": {
        "id": "pt4vvN2dTY4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!7z a dataset.7z dataset/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mqhi8avbTTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp dataset.7z /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "ss4VoAfUTpqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}