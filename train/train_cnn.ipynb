{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/rediahmds/eco-sort/blob/main/train/train_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"## Connect to Google Drive","metadata":{"id":"Hi1UbGUd2K1n"}},{"cell_type":"code","source":"USE_CLEARML = False\nUSE_GOOGLE_COLAB = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T06:09:33.811949Z","iopub.execute_input":"2025-08-17T06:09:33.812524Z","iopub.status.idle":"2025-08-17T06:09:33.815731Z","shell.execute_reply.started":"2025-08-17T06:09:33.812499Z","shell.execute_reply":"2025-08-17T06:09:33.815057Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if USE_GOOGLE_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\nelse:\n    !pip install PyDrive2\n\n    from pydrive2.auth import GoogleAuth\n    from pydrive2.drive import GoogleDrive\n\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    GCP_CLIENT_SECRET = user_secrets.get_secret(\"GCP_CLIENT_SECRET\")\n\n    with open(\"client_secrets.json\", \"w\") as f:\n        f.write(GCP_CLIENT_SECRET)\n\n\n    gauth = GoogleAuth()\n    gauth.CommandLineAuth()\n\n    drive = GoogleDrive(gauth)\n\n    def save_to_gdrive(name: str, destination_dir_id: str):\n        pass","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htmo7u-i2J65","outputId":"eefefe31-2c91-4e31-8164-5a323fb19792","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:44:05.885042Z","iopub.execute_input":"2025-08-17T04:44:05.885737Z","iopub.status.idle":"2025-08-17T04:44:27.467460Z","shell.execute_reply.started":"2025-08-17T04:44:05.885701Z","shell.execute_reply":"2025-08-17T04:44:27.466654Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyDrive2 in /usr/local/lib/python3.11/dist-packages (1.21.3)\nRequirement already satisfied: google-api-python-client>=1.12.5 in /usr/local/lib/python3.11/dist-packages (from PyDrive2) (2.173.0)\nRequirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive2) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive2) (6.0.2)\nRequirement already satisfied: cryptography<44 in /usr/local/lib/python3.11/dist-packages (from PyDrive2) (43.0.3)\nRequirement already satisfied: pyOpenSSL<=24.2.1,>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive2) (24.2.1)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44->PyDrive2) (1.17.1)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.22.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.5->PyDrive2) (2.40.3)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.5->PyDrive2) (1.34.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.5->PyDrive2) (4.2.0)\nRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive2) (0.6.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive2) (0.4.2)\nRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive2) (4.9.1)\nRequirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive2) (1.17.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44->PyDrive2) (2.22)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.70.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2.32.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->PyDrive2) (5.5.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.12.5->PyDrive2) (3.0.9)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2025.6.15)\nGo to the following link in your browser:\n\n    https://accounts.google.com/o/oauth2/auth?client_id=991309652978-hpmqspbdt0bccglf6iklc3er1d9naa8q.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=online&response_type=code\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter verification code:  4/1AVMBsJgC73mrKqFVsFAiZZhT0GeIR5GP05WJmNcvWhFaBr7QJG1d1CWyl3U\n"},{"name":"stdout","text":"Authentication successful.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Prepare dataset (Run only once)\n\nRun for the first time only. when starting new session, dont run it.","metadata":{"id":"HYavOwVuEkGf"}},{"cell_type":"markdown","source":"### Download","metadata":{"id":"oCK3f0-SFms6"}},{"cell_type":"code","source":"import shutil\nimport os\n\ndef copy_file_to_folder(source_path: str, destination_folder: str, preserve_metadata: bool = False):\n    \"\"\"\n    Copies a file to a specified folder, creating the folder if it doesn't exist.\n\n    Args:\n        source_path (str): The full path to the source file.\n        destination_folder (str): The path to the destination folder.\n        preserve_metadata (bool): If True, preserves file metadata (uses shutil.copy2).\n                                  Defaults to False.\n\n    Returns:\n        bool: True if the copy was successful, False otherwise.\n    \"\"\"\n    try:\n        # Create the destination folder if it doesn't exist\n        os.makedirs(destination_folder, exist_ok=True)\n\n        # Choose the copy function based on the preserve_metadata flag\n        if preserve_metadata:\n            shutil.copy2(source_path, destination_folder)\n        else:\n            shutil.copy(source_path, destination_folder)\n\n        print(f\"File '{source_path}' copied successfully to '{destination_folder}'. ✅\")\n\n        return True\n\n    except FileNotFoundError:\n        print(f\"Error: The source file was not found at '{source_path}'.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False","metadata":{"id":"wGsUlIFaIND-","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:44:38.053138Z","iopub.execute_input":"2025-08-17T04:44:38.053890Z","iopub.status.idle":"2025-08-17T04:44:38.059477Z","shell.execute_reply.started":"2025-08-17T04:44:38.053858Z","shell.execute_reply":"2025-08-17T04:44:38.058922Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if USE_GOOGLE_COLAB:\n    DATASET_PATH = \"/content/drive/MyDrive/Cool Lee Yeah/8th Semester/Skripsi/Datasets/dataset_no-styro.7z\"\n    copy_file_to_folder(DATASET_PATH, \"/content/\")\nelse:\n    GDRIVE_DATASET_FILE_ID = user_secrets.get_secret(\"GDRIVE_DATASET_FILE_ID\")\n    file_id = GDRIVE_DATASET_FILE_ID\n    local_filename = \"dataset.7z\" # save as\n\n    try:\n        file_to_download = drive.CreateFile({'id': file_id})\n\n        print(f\"⬇️ Downloading file: '{file_to_download['title']}'...\")\n        file_to_download.GetContentFile(local_filename)\n\n        print(f\"✅ Downloaded successfully and saved as '{local_filename}'\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")","metadata":{"id":"3JmiaTh-JqS9","outputId":"a8853a9a-6860-4ca8-a233-e1dc6ff39815","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:44:55.836112Z","iopub.execute_input":"2025-08-17T04:44:55.836586Z","iopub.status.idle":"2025-08-17T04:45:19.037391Z","shell.execute_reply.started":"2025-08-17T04:44:55.836561Z","shell.execute_reply":"2025-08-17T04:45:19.036758Z"}},"outputs":[{"name":"stdout","text":"⬇️ Downloading file: 'dataset_no-styro.7z'...\n✅ Downloaded successfully and saved as 'dataset.7z'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Extract the Dataset","metadata":{"id":"SNy902YmKCeq"}},{"cell_type":"code","source":"!7z x dataset.7z","metadata":{"id":"nt-XR1nuKIss","outputId":"b932b99f-dec0-4b18-9e94-4fec7d0512b1","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:46:11.130561Z","iopub.execute_input":"2025-08-17T04:46:11.131254Z","iopub.status.idle":"2025-08-17T04:47:18.675418Z","shell.execute_reply.started":"2025-08-17T04:46:11.131225Z","shell.execute_reply":"2025-08-17T04:47:18.674691Z"}},"outputs":[{"name":"stdout","text":"\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Sca        1 file, 1234923334 bytes (1178 MiB)\n\nExtracting archive: dataset.7z\n--\nPath = dataset.7z\nType = 7z\nPhysical Size = 1234923334\nHeaders Size = 178477\nMethod = LZMA2:24\nSolid = +\nBlocks = 1\n\n      0% 9 - dataset/train/background/0.jp                                        0% 582 - dataset/train/background/158.jp                                            0% 828 - dataset/train/background/38.j                                          0% 1211 - dataset/train/background/724.j                                            0% 1363 - dataset/train/background/861.j                                            1% 1532 - dataset/train/glass/Glass_113.jp                                              1% 1608 - dataset/train/glass/Glass_182.jp                                              2% 1650 - dataset/train/glass/Glass_22.j                                            2% 1725 - dataset/train/glass/Glass_288.jp                                              3% 1763 - dataset/train/glass/Glass_321.jp                                              3% 1836 - dataset/train/glass/Glass_388.jp                                              4% 1889 - dataset/train/glass/Glass_57.j                                            5% 1923 - dataset/train/glass/Glass_88.j                                            5% 2234 - dataset/train/glass/brown-glass371.j                                                  5% 2565 - dataset/train/glass/default_gla . age_bottles_0048__Image_175.pn                                                                              5% 2620 - dataset/train/glass/default_gla . age_bottles_0131__Image_193.pn                                                                              6% 2683 - dataset/train/glass/default_gla . age_bottles_0239__Image_250.pn                                                                              6% 2752 - dataset/train/glass/default_glas . c_containers_0108__Image_167.pn                                                                                6% 2831 - dataset/train/glass/default_glas . c_containers_0238__Image_230.pn                                                                                6% 2902 - dataset/train/glass/default_glass_food_jars_0097__Image_121.pn                                                                            7% 2967 - dataset/train/glass/default_glass_food_jars_0197__Image_202.pn                                                                            7% 3172 - dataset/train/glass/green-glass271.j                                                  7% 3304 - dataset/train/glass/green-glass395.j                                                  7% 3446 - dataset/train/glass/green-glass541.j                                                  7% 3619 - dataset/train/glass/real_world_ . erage_bottles_0088__Image_67.p                                                                              8% 3694 - dataset/train/glass/real_world_ . erage_bottles_0206__Image_17.p                                                                              8% 3775 - dataset/train/glass/real_worl . containers_0099__Image_51.pn                                                                          8% 3825 - dataset/train/glass/real_worl . containers_0188__Image_179.p                                                                          9% 3857 - dataset/train/glass/real_worl . containers_0243__Image_98.pn                                                                          9% 3932 - dataset/train/glass/real_world_glass_food_jars_0110__Image_59.pn                                                                              9% 4000 - dataset/train/glass/real_world_glass_food_jars_0205__Image_95.pn                                                                              9% 4039 - dataset/train/glass/white-glass112.j                                                  9% 4220 - dataset/train/glass/white-glass329.j                                                  9% 4339 - dataset/train/glass/white-glass467.j                                                 10% 4458 - dataset/train/glass/white-glass617.j                                                 10% 4617 - dataset/train/glass/white-glass91.jp                                                 10% 4673 - dataset/train/metal/Metal_145.jp                                             11% 4712 - dataset/train/metal/Metal_182.jp                                             11% 4776 - dataset/train/metal/Metal_242.jp                                             12% 4836 - dataset/train/metal/Metal_299.jp                                             12% 4898 - dataset/train/metal/Metal_357.jp                                             13% 4946 - dataset/train/metal/Metal_402.jp                                             13% 5000 - dataset/train/metal/Metal_454.jp                                             14% 5035 - dataset/train/metal/Metal_488.jp                                             14% 5102 - dataset/train/metal/Metal_55.j                                           15% 5135 - dataset/train/metal/Metal_580.jp                                             15% 5203 - dataset/train/metal/Metal_649.jp                                             16% 5239 - dataset/train/metal/Metal_682.jp                                             16% 5313 - dataset/train/metal/Metal_754.jp                                             17% 5353 - dataset/train/metal/Metal_8.jp                                           17% 5384 - dataset/train/metal/default_aerosol_cans_0014__Image_156.p                                                                       18% 5461 - dataset/train/metal/default_aerosol_cans_0141__Image_71.pn                                                                       18% 5538 - dataset/train/metal/default_aluminum_food_cans_0034__Image_245.p                                                                             18% 5614 - dataset/train/metal/default_aluminum_food_cans_0154__Image_246.p                                                                             18% 5684 - dataset/train/metal/default_aluminum_soda_cans_0019__Image_160.p                                                                             19% 5685 - dataset/train/metal/default_aluminum_soda_cans_0020__Image_120.p                                                                             19% 5754 - dataset/train/metal/default_aluminum_soda_cans_0125__Image_29.pn                                                                             19% 5827 - dataset/train/metal/default_aluminum_soda_cans_0238__Image_230.p                                                                             19% 5901 - dataset/train/metal/default_steel_food_cans_0134__Image_218.pn                                                                           20% 5977 - dataset/train/metal/metal108.j                                           20% 6013 - dataset/train/metal/metal140.j                                           20% 6265 - dataset/train/metal/metal372.j                                           20% 6285 - dataset/train/metal/metal391.j                                           20% 6547 - dataset/train/metal/metal645.j                                           21% 6702 - dataset/train/metal/metal98.jp                                           21% 6773 - dataset/train/metal/real_world_aerosol_cans_0119__Image_236.pn                                                                           21% 6809 - dataset/train/metal/real_world_aerosol_cans_0186__Image_242.pn                                                                           21% 6844 - dataset/train/metal/real_world . m_food_cans_0004__Image_37.pn                                                                           22% 6903 - dataset/train/metal/real_world . m_food_cans_0115__Image_142.p                                                                           22% 6968 - dataset/train/metal/real_world . m_food_cans_0239__Image_250.p                                                                           22% 7015 - dataset/train/metal/real_world . m_soda_cans_0059__Image_183.p                                                                           22% 7027 - dataset/train/metal/real_world . m_soda_cans_0080__Image_31.pn                                                                           23% 7084 - dataset/train/metal/real_world . m_soda_cans_0166__Image_10.pn                                                                           23% 7143 - dataset/train/metal/real_world . m_soda_cans_0243__Image_98.pn                                                                           23% 7186 - dataset/train/metal/real_world_steel_food_cans_0071__Image_82.pn                                                                             23% 7206 - dataset/train/metal/real_world_steel_food_cans_0110__Image_59.pn                                                                             24% 7274 - dataset/train/metal/real_world_steel_food_cans_0240__Image_36.pn                                                                             24% 7349 - dataset/train/organic/Food Organics_163.jp                                                       25% 7395 - dataset/train/organic/Food Organics_204.jp                                                       26% 7441 - dataset/train/organic/Food Organics_246.jp                                                       26% 7462 - dataset/train/organic/Food Organics_265.jp                                                       27% 7496 - dataset/train/organic/Food Organics_296.jp                                                       27% 7524 - dataset/train/organic/Food Organics_320.jp                                                       28% 7546 - dataset/train/organic/Food Organics_340.jp                                                       28% 7587 - dataset/train/organic/Food Organics_378.jp                                                       28% 7606 - dataset/train/organic/Food Organics_395.jp                                                       29% 7630 - dataset/train/organic/Food Organics_46.j                                                     30% 7681 - dataset/train/organic/Food Organics_93.j                                                     30% 7701 - dataset/train/organic/Vegetation_110.j                                                   30% 7721 - dataset/train/organic/Vegetation_129.j                                                   31% 7740 - dataset/train/organic/Vegetation_146.j                                                   31% 7760 - dataset/train/organic/Vegetation_164.j                                                   31% 7779 - dataset/train/organic/Vegetation_181.j                                                   32% 7798 - dataset/train/organic/Vegetation_199.j                                                   32% 7836 - dataset/train/organic/Vegetation_232.j                                                   33% 7877 - dataset/train/organic/Vegetation_27.jp                                                   34% 7920 - dataset/train/organic/Vegetation_308.j                                                   34% 7941 - dataset/train/organic/Vegetation_327.j                                                   34% 7963 - dataset/train/organic/Vegetation_347.j                                                   35% 8003 - dataset/train/organic/Vegetation_383.j                                                   36% 8046 - dataset/train/organic/Vegetation_421.j                                                   36% 8068 - dataset/train/organic/Vegetation_49.jp                                                   36% 8089 - dataset/train/organic/Vegetation_68.jp                                                   37% 8109 - dataset/train/organic/Vegetation_86.jp                                                   37% 8225 - dataset/train/organic/biological190.jp                                                   37% 8569 - dataset/train/organic/biological504.jp                                                   37% 8721 - dataset/train/organic/biological642.jp                                                   37% 8889 - dataset/train/organic/biological796.jp                                                   38% 9105 - dataset/train/organic/default_coffee_grounds_0014__Image_156.p                                                                           38% 9155 - dataset/train/organic/default_coffee_grounds_0107__Image_33.pn                                                                           38% 9164 - dataset/train/organic/default_coffee_grounds_0128__Image_231.p                                                                           38% 9205 - dataset/train/organic/default_coffee_grounds_0201__Image_53.pn                                                                           39% 9259 - dataset/train/organic/default_eggshells_0047__Image_200.pn                                                                       39% 9313 - dataset/train/organic/default_eggshells_0128__Image_231.pn                                                                       39% 9370 - dataset/train/organic/default_eggshells_0214__Image_68.p                                                                     40% 9425 - dataset/train/organic/default_food_waste_0047__Image_200.p                                                                       40% 9477 - dataset/train/organic/default_food_waste_0143__Image_178.p                                                                       40% 9528 - dataset/train/organic/default_tea_bags_0004__Image_37.pn                                                                     41% 9584 - dataset/train/organic/default_tea_bags_0095__Image_182.p                                                                     41% 9648 - dataset/train/organic/default_tea_bags_0182__Image_83.pn                                                                     41% 9761 - dataset/train/organic/organic_005205_photo.j                                                         41% 9819 - dataset/train/organic/organic_009003_photo.j                                                         42% 9925 - dataset/train/organic/real_world_coffee_grounds_0074__Image_212.pn                                                                               42% 9947 - dataset/train/organic/real_world_coffee_grounds_0107__Image_33.p                                                                             42% 9974 - dataset/train/organic/real_world_coffee_grounds_0155__Image_113.pn                                                                               42% 10024 - dataset/train/organic/real_world_coffee_grounds_0247__Image_60.pn                                                                               43% 10085 - dataset/train/organic/real_world_eggshells_0096__Image_194.pn                                                                           43% 10141 - dataset/train/organic/real_world_eggshells_0186__Image_242.pn                                                                           43% 10174 - dataset/train/organic/real_world_eggshells_0229__Image_240.pn                                                                           44% 10196 - dataset/train/organic/real_world_food_waste_0024__Image_217.p                                                                           44% 10247 - dataset/train/organic/real_world_food_waste_0131__Image_193.p                                                                           44% 10299 - dataset/train/organic/real_world_food_waste_0228__Image_239.p                                                                           44% 10354 - dataset/train/organic/real_world_tea_bags_0058__Image_5.p                                                                       45% 10386 - dataset/train/organic/real_world_tea_bags_0102__Image_247.p                                                                         45% 10410 - dataset/train/organic/real_world_tea_bags_0138__Image_129.p                                                                         45% 10474 - dataset/train/organic/real_world_tea_bags_0228__Image_239.p                                                                         46% 10508 - dataset/train/paper/Cardboard_117.j                                                 46% 10562 - dataset/train/paper/Cardboard_166.j                                                 47% 10627 - dataset/train/paper/Cardboard_224.j                                                 47% 10666 - dataset/train/paper/Cardboard_26.jp                                                 48% 10742 - dataset/train/paper/Cardboard_328.j                                                 48% 10779 - dataset/train/paper/Cardboard_361.j                                                 49% 10855 - dataset/train/paper/Cardboard_43.jp                                                 49% 10893 - dataset/train/paper/Cardboard_49.jp                                                 50% 10955 - dataset/train/paper/Paper_104.j                                             50% 10985 - dataset/train/paper/Paper_131.j                                             51% 11056 - dataset/train/paper/Paper_196.j                                             51% 11092 - dataset/train/paper/Paper_228.j                                             52% 11158 - dataset/train/paper/Paper_288.j                                             52% 11197 - dataset/train/paper/Paper_322.j                                             53% 11254 - dataset/train/paper/Paper_374.j                                             53% 11284 - dataset/train/paper/Paper_400.j                                             54% 11336 - dataset/train/paper/Paper_448.j                                             54% 11403 - dataset/train/paper/Paper_58.jp                                             55% 11436 - dataset/train/paper/Paper_88.jp                                             55% 11562 - dataset/train/paper/cardboard299.jp                                                 55% 11682 - dataset/train/paper/cardboard49.j                                               55% 11968 - dataset/train/paper/default_cardboard_boxes_0054__Image_28.pn                                                                           56% 12052 - dataset/train/paper/default_cardboard_boxes_0222__Image_72.pn                                                                           56% 12133 - dataset/train/paper/default_c . _packaging_0161__Image_206.pn                                                                           56% 12195 - dataset/train/paper/default_magazines_0018__Image_126.p                                                                     57% 12240 - dataset/train/paper/default_magazines_0088__Image_67.pn                                                                     57% 12325 - dataset/train/paper/default_magazines_0204__Image_52.pn                                                                     58% 12370 - dataset/train/paper/default_newspaper_0017__Image_103.p                                                                     58% 12371 - dataset/train/paper/default_newspaper_0018__Image_126.p                                                                     58% 12423 - dataset/train/paper/default_newspaper_0080__Image_31.pn                                                                     59% 12477 - dataset/train/paper/default_newspaper_0145__Image_134.p                                                                     59% 12527 - dataset/train/paper/default_newspaper_0208__Image_128.p                                                                     59% 12582 - dataset/train/paper/default_office_paper_0021__Image_13.p                                                                       59% 12583 - dataset/train/paper/default_office_paper_0022__Image_40.p                                                                       60% 12655 - dataset/train/paper/default_office_paper_0115__Image_142.pn                                                                         60% 12723 - dataset/train/paper/default_office_paper_0199__Image_151.pn                                                                         60% 12794 - dataset/train/paper/default_paper_cups_0029__Image_57.p                                                                     60% 12809 - dataset/train/paper/default_paper_cups_0059__Image_183.pn                                                                       61% 12869 - dataset/train/paper/default_paper_cups_0147__Image_173.pn                                                                       61% 12980 - dataset/train/paper/paper168.jp                                             61% 13024 - dataset/train/paper/paper262.jp                                             61% 13204 - dataset/train/paper/paper572.jp                                             61% 13233 - dataset/train/paper/paper652.jp                                             61% 13323 - dataset/train/paper/paper827.jp                                             62% 13429 - dataset/train/paper/real_world_cardboard_boxes_0010__Image_138.pn                                                                               62% 13488 - dataset/train/paper/real_world_cardboard_boxes_0102__Image_247.pn                                                                               62% 13495 - dataset/train/paper/real_world_cardboard_boxes_0111__Image_169.pn                                                                               62% 13568 - dataset/train/paper/real_world_cardboard_boxes_0222__Image_72.p                                                                             62% 13619 - dataset/train/paper/real_world_ . d_packaging_0046__Image_124.p                                                                             63% 13642 - dataset/train/paper/real_world . rd_packaging_0088__Image_67.pn                                                                             63% 13720 - dataset/train/paper/real_world_ . d_packaging_0202__Image_190.p                                                                             63% 13777 - dataset/train/paper/real_world_magazines_0035__Image_58.p                                                                       63% 13824 - dataset/train/paper/real_world_magazines_0096__Image_194.pn                                                                         64% 13867 - dataset/train/paper/real_world_magazines_0150__Image_79.p                                                                       64% 13903 - dataset/train/paper/real_world_magazines_0204__Image_52.p                                                                       64% 13929 - dataset/train/paper/real_world_magazines_0235__Image_4.pn                                                                       65% 13962 - dataset/train/paper/real_world_newspaper_0021__Image_13.p                                                                       65% 14016 - dataset/train/paper/real_world_newspaper_0083__Image_244.pn                                                                         65% 14032 - dataset/train/paper/real_world_newspaper_0105__Image_147.pn                                                                         65% 14071 - dataset/train/paper/real_world_newspaper_0150__Image_79.p                                                                       66% 14125 - dataset/train/paper/real_world_newspaper_0211__Image_141.pn                                                                         66% 14150 - dataset/train/paper/real_world_newspaper_0237__Image_237.pn                                                                         66% 14188 - dataset/train/paper/real_world_office_paper_0037__Image_25.pn                                                                           66% 14245 - dataset/train/paper/real_world_office_paper_0116__Image_32.pn                                                                           66% 14248 - dataset/train/paper/real_world_office_paper_0120__Image_143.p                                                                           67% 14312 - dataset/train/paper/real_world_office_paper_0208__Image_128.p                                                                           67% 14375 - dataset/train/paper/real_world_paper_cups_0038__Image_101.p                                                                         67% 14386 - dataset/train/paper/real_world_paper_cups_0051__Image_55.pn                                                                         67% 14451 - dataset/train/paper/real_world_paper_cups_0136__Image_222.p                                                                         67% 14487 - dataset/train/paper/real_world_paper_cups_0185__Image_107.p                                                                         68% 14526 - dataset/train/paper/real_world_paper_cups_0242__Image_161.p                                                                         68% 14593 - dataset/train/plastic/Plastic_156.j                                                 69% 14661 - dataset/train/plastic/Plastic_217.j                                                 69% 14697 - dataset/train/plastic/Plastic_250.j                                                 70% 14754 - dataset/train/plastic/Plastic_301.j                                                 71% 14808 - dataset/train/plastic/Plastic_350.j                                                 71% 14856 - dataset/train/plastic/Plastic_394.j                                                 72% 14911 - dataset/train/plastic/Plastic_443.j                                                 73% 14973 - dataset/train/plastic/Plastic_5.j                                               73% 15023 - dataset/train/plastic/Plastic_544.j                                                 74% 15078 - dataset/train/plastic/Plastic_594.j                                                 75% 15132 - dataset/train/plastic/Plastic_642.j                                                 75% 15185 - dataset/train/plastic/Plastic_690.j                                                 76% 15236 - dataset/train/plastic/Plastic_736.j                                                 77% 15290 - dataset/train/plastic/Plastic_785.j                                                 77% 15327 - dataset/train/plastic/Plastic_818.j                                                 78% 15395 - dataset/train/plastic/Plastic_88.jp                                                 78% 15428 - dataset/train/plastic/Plastic_909.j                                                 78% 15482 - dataset/train/plastic/default_plastic_cup_lids_0066__Image_227.pn                                                                               79% 15554 - dataset/train/plastic/default_plastic_cup_lids_0178__Image_38.p                                                                             79% 15650 - dataset/train/plastic/default . t_bottles_0090__Image_228.p                                                                         79% 15760 - dataset/train/plastic/defaul . ntainers_0008__Image_150.p                                                                       80% 15824 - dataset/train/plastic/default_p . od_containers_0125__Image_29.pn                                                                               80% 15888 - dataset/train/plastic/default_p . od_containers_0240__Image_36.pn                                                                               80% 15954 - dataset/train/plastic/default_p . hopping_bags_0096__Image_194.pn                                                                               81% 16026 - dataset/train/plastic/default_ . shopping_bags_0235__Image_4.pn                                                                             81% 16106 - dataset/train/plastic/default_ . soda_bottles_0141__Image_71.pn                                                                             82% 16219 - dataset/train/plastic/default_plastic_straws_0090__Image_228.pn                                                                             82% 16281 - dataset/train/plastic/default_plastic_straws_0191__Image_232.pn                                                                             82% 16340 - dataset/train/plastic/default . _trash_bags_0062__Image_11.pn                                                                           83% 16404 - dataset/train/plastic/default . _trash_bags_0201__Image_53.pn                                                                           83% 16481 - dataset/train/plastic/default_p . ater_bottles_0087__Image_210.pn                                                                               83% 16486 - dataset/train/plastic/default_p . ater_bottles_0092__Image_197.pn                                                                               84% 16555 - dataset/train/plastic/default_p . ater_bottles_0192__Image_94.p                                                                             84% 16634 - dataset/train/plastic/real_wor . ic_cup_lids_0082__Image_20.p                                                                           84% 16708 - dataset/train/plastic/real_wor . ic_cup_lids_0218__Image_211.pn                                                                             84% 16791 - dataset/train/plastic/real_wor . ent_bottles_0095__Image_182.pn                                                                             85% 16824 - dataset/train/plastic/real_wor . ent_bottles_0143__Image_178.pn                                                                             85% 16901 - dataset/train/plastic/real_wo . containers_0016__Image_137.pn                                                                           85% 16961 - dataset/train/plastic/real_wo . containers_0129__Image_86.p                                                                         85% 17018 - dataset/train/plastic/real_wo . containers_0231__Image_75.p                                                                         86% 17027 - dataset/train/plastic/real_worl . _shopping_bags_0003__Image_6.pn                                                                               86% 17090 - dataset/train/plastic/real_w . ping_bags_0109__Image_154.pn                                                                         86% 17146 - dataset/train/plastic/real_w . ping_bags_0206__Image_17.p                                                                       86% 17208 - dataset/train/plastic/real_worl . c_soda_bottles_0058__Image_5.pn                                                                               87% 17266 - dataset/train/plastic/real_worl . _soda_bottles_0166__Image_10.pn                                                                               87% 17326 - dataset/train/plastic/real_wo . tic_straws_0018__Image_126.pn                                                                           87% 17365 - dataset/train/plastic/real_world_plastic_straws_0084__Image_56.pn                                                                               88% 17383 - dataset/train/plastic/real_world_plastic_straws_0107__Image_33.pn                                                                               88% 17442 - dataset/train/plastic/real_wo . tic_straws_0203__Image_105.pn                                                                           88% 17497 - dataset/train/plastic/real_worl . c_trash_bags_0034__Image_245.pn                                                                               88% 17498 - dataset/train/plastic/real_worl . c_trash_bags_0036__Image_166.pn                                                                               89% 17558 - dataset/train/plastic/real_worl . c_trash_bags_0125__Image_29.p                                                                             89% 17621 - dataset/train/plastic/real_worl . c_trash_bags_0237__Image_237.pn                                                                               89% 17644 - dataset/train/plastic/real_w . r_bottles_0023__Image_69.p                                                                       89% 17676 - dataset/train/plastic/real_w . r_bottles_0069__Image_140.pn                                                                         89% 17736 - dataset/train/plastic/real_w . r_bottles_0163__Image_74.p                                                                       90% 17761 - dataset/train/plastic/real_w . r_bottles_0195__Image_26.p                                                                       90% 17794 - dataset/train/textiles/clothes1.j                                               90% 17935 - dataset/train/textiles/clothes1419.jp                                                   90% 18084 - dataset/train/textiles/clothes1866.jp                                                   91% 18153 - dataset/train/textiles/clothes2107.jp                                                   91% 18233 - dataset/train/textiles/clothes2349.jp                                                   91% 18379 - dataset/train/textiles/clothes2824.jp                                                   91% 18523 - dataset/train/textiles/clothes3295.jp                                                   92% 18603 - dataset/train/textiles/clothes352.j                                                 92% 18670 - dataset/train/textiles/clothes3761.jp                                                   92% 18815 - dataset/train/textiles/clothes4257.jp                                                   92% 18957 - dataset/train/textiles/clothes470.j                                                 93% 19052 - dataset/train/textiles/clothes5005.jp                                                   93% 19112 - dataset/train/textiles/clothes5180.jp                                                   93% 19267 - dataset/train/textiles/clothes914.j                                                 93% 19335 - dataset/train/textiles/default_clothing_0043__Image_208.p                                                                       94% 19389 - dataset/train/textiles/default_clothing_0099__Image_51.pn                                                                       94% 19443 - dataset/train/textiles/default_clothing_0154__Image_246.p                                                                       94% 19498 - dataset/train/textiles/default_clothing_0212__Image_8.p                                                                     95% 19556 - dataset/train/textiles/default_shoes_0023__Image_69.p                                                                   95% 19621 - dataset/train/textiles/default_shoes_0090__Image_228.pn                                                                     95% 19642 - dataset/train/textiles/default_shoes_0111__Image_169.pn                                                                     96% 19688 - dataset/train/textiles/default_shoes_0157__Image_48.p                                                                   96% 19752 - dataset/train/textiles/default_shoes_0222__Image_72.p                                                                   96% 19814 - dataset/train/textiles/real_world_clothing_0041__Image_39.p                                                                         96% 19857 - dataset/train/textiles/real_world_clothing_0093__Image_177.pn                                                                           97% 19891 - dataset/train/textiles/real_world_clothing_0129__Image_86.p                                                                         97% 19926 - dataset/train/textiles/real_world_clothing_0171__Image_34.p                                                                         97% 19981 - dataset/train/textiles/real_world_clothing_0233__Image_35.p                                                                         98% 20041 - dataset/train/textiles/real_world_shoes_0055__Image_102.p                                                                       98% 20104 - dataset/train/textiles/real_world_shoes_0126__Image_63.pn                                                                       98% 20155 - dataset/train/textiles/real_world_shoes_0193__Image_50.pn                                                                       98% 20166 - dataset/train/textiles/real_world_shoes_0206__Image_17.pn                                                                       98% 20319 - dataset/train/textiles/shoes1134.jp                                                 99% 20394 - dataset/train/textiles/shoes1230.jp                                                 99% 20486 - dataset/train/textiles/shoes1336.jp                                                 99% 20663 - dataset/train/textiles/shoes1553.jp                                                 99% 20776 - dataset/train/textiles/shoes1691.jp                                                 99% 20809 - dataset/train/textiles/shoes1727.jp                                                 99% 20956 - dataset/train/textiles/shoes1905.jp                                                 99% 21005 - dataset/train/textiles/shoes1967.jp                                                 99% 21094 - dataset/train/textiles/shoes287.j                                               99% 21239 - dataset/train/textiles/shoes461.j                                               99% 21379 - dataset/train/textiles/shoes621.j                                               99% 21514 - dataset/train/textiles/shoes78.jp                                               99% 21586 - dataset/train/textiles/shoes876.j                                              Everything is Ok\n\nFolders: 9\nFiles: 21677\nSize:       1247128906\nCompressed: 1234923334\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Show directory tree","metadata":{"id":"of2TWdqofglL"}},{"cell_type":"markdown","source":"### Create Validation Dataset\n\nThis dataset will be created by moving some files from training dataset.","metadata":{"id":"Ml-cAB1aAMS1"}},{"cell_type":"code","source":"from pathlib import Path\nimport random\nfrom tqdm import tqdm\nimport shutil\n\nimport os\n\ndef count_files_in_folders(base_path):\n  \"\"\"\n  Counts the number of files in each subfolder of a given path.\n\n  Args:\n    base_path (str): The path to the parent directory (e.g., 'dataset/train').\n\n  Returns:\n    dict: A dictionary with subfolder names as keys and file counts as values.\n          Returns None if the path is not found.\n  \"\"\"\n  if not os.path.isdir(base_path):\n    print(f\"Error: Directory not found at '{base_path}'\")\n    return None\n\n  counts_per_class = {}\n  # Loop through each item in the base directory\n  for class_name in os.listdir(base_path):\n    class_path = os.path.join(base_path, class_name)\n    # Ensure it is a directory\n    if os.path.isdir(class_path):\n      # Count the number of files inside the subdirectory and store it\n      file_count = len(os.listdir(class_path))\n      counts_per_class[class_name] = file_count\n  return counts_per_class\n\ndef calculate_dataset_split(counts_per_class, percentage):\n  \"\"\"\n  Calculates the number of items per class for a split based on a percentage.\n  \"\"\"\n  if not 0 <= percentage <= 100:\n    raise ValueError(\"Percentage must be between 0 and 100.\")\n\n  split_counts_result = {}\n  for class_name, total_count in counts_per_class.items():\n    split_count = total_count * (percentage / 100)\n    # Round to the nearest integer as file counts cannot be fractional\n    split_counts_result[class_name] = round(split_count)\n  return split_counts_result\n\ndef move_validation_split_custom(train_dir, val_dir, per_class_counts: dict, random_select=True):\n    train_dir = Path(train_dir)\n    val_dir = Path(val_dir)\n    val_dir.mkdir(parents=True, exist_ok=True)\n\n    for class_name, n in per_class_counts.items():\n        class_dir = train_dir / class_name\n        if not class_dir.exists():\n            print(f\"⚠️ Folder tidak ditemukan: {class_dir}\")\n            continue\n\n        images = sorted([p for p in class_dir.glob(\"*.*\") if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n        selected = random.sample(images, min(n, len(images))) if random_select else images[:n]\n        val_class_dir = val_dir / class_name\n        val_class_dir.mkdir(parents=True, exist_ok=True)\n\n        print(f\"📁 {class_name}: Memindahkan {len(selected)} file...\")\n        for img in tqdm(selected, desc=f\"  Pindah {class_name}\", leave=False):\n            shutil.move(str(img), str(val_class_dir / img.name))\n\n    print(\"\\n✅ Selesai membuat validasi set proporsional.\")\n","metadata":{"id":"XqGECqIqAg-p","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:48:00.533273Z","iopub.execute_input":"2025-08-17T04:48:00.533974Z","iopub.status.idle":"2025-08-17T04:48:00.548599Z","shell.execute_reply.started":"2025-08-17T04:48:00.533937Z","shell.execute_reply":"2025-08-17T04:48:00.548077Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Divide Dataset\n\nThere are two methods for this, `StratifiedShuffle` and `train_test_split`","metadata":{"id":"c2PqRXewJeuF"}},{"cell_type":"code","source":"from torchvision import datasets\n\nDATASET_PATH = \"dataset/train\"\nfull_dataset = datasets.ImageFolder(DATASET_PATH)\nclass_names = full_dataset.classes","metadata":{"id":"P-XfE16hRTkO","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:52:34.096524Z","iopub.execute_input":"2025-08-17T04:52:34.096780Z","iopub.status.idle":"2025-08-17T04:52:34.149510Z","shell.execute_reply.started":"2025-08-17T04:52:34.096763Z","shell.execute_reply":"2025-08-17T04:52:34.148941Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### (1) Divide: Stratified Method","metadata":{"id":"_aWo6ngmJ67p"}},{"cell_type":"code","source":"from pathlib import Path\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nimport shutil\nfrom tqdm import tqdm\n\ndef stratified_split_imagefolder(source_dir: Path | str,\n                                  target_train: Path | str,\n                                  target_val: Path | str,\n                                  val_ratio=0.2,\n                                  random_state=42):\n    source_dir = Path(source_dir)\n    target_train = Path(target_train)\n    target_val = Path(target_val)\n    target_train.mkdir(parents=True, exist_ok=True)\n    target_val.mkdir(parents=True, exist_ok=True)\n\n    transform = transforms.Compose([transforms.ToTensor()])\n    dataset = ImageFolder(str(source_dir), transform=transform)\n\n    paths = [Path(p) for p, _ in dataset.samples]\n    labels = [label for _, label in dataset.samples]\n    class_names = dataset.classes\n\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n    train_idx, val_idx = next(sss.split(paths, labels))\n\n    print(f\"📦 Total gambar: {len(paths)}\")\n    print(f\"✅ Train: {len(train_idx)}\")\n    print(f\"✅ Val  : {len(val_idx)}\")\n\n    def copy_split(index_list, target_root):\n        for idx in tqdm(index_list, desc=f\"Salin ke {target_root.name}\"):\n            src = paths[idx]\n            label_name = class_names[labels[idx]]\n            dest_dir = target_root / label_name\n            dest_dir.mkdir(parents=True, exist_ok=True)\n            shutil.copy(src, dest_dir / src.name)\n\n    copy_split(train_idx, target_train)\n    copy_split(val_idx, target_val)\n\n    print(\"✅ Stratified split selesai.\")\n","metadata":{"id":"HqOw6PC-SVXo","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T07:34:57.682300Z","iopub.execute_input":"2025-08-01T07:34:57.682801Z","iopub.status.idle":"2025-08-01T07:34:58.191992Z","shell.execute_reply.started":"2025-08-01T07:34:57.682777Z","shell.execute_reply":"2025-08-01T07:34:58.191415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stratified_split_imagefolder(\n    source_dir=\"dataset/train\",\n    target_train=\"dataset_stratified/train\",\n    target_val=\"dataset_stratified/test\",\n    val_ratio=0.2,\n    random_state=42\n)","metadata":{"id":"DpgKX4XNORyn","outputId":"245bfe34-7c14-40ce-e87f-dd5b640509de","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["📦 Total gambar: 22198\n","✅ Train: 17758\n","✅ Val  : 4440\n"]},{"output_type":"stream","name":"stderr","text":["Salin ke train: 100%|██████████| 17758/17758 [00:09<00:00, 1895.10it/s]\n","Salin ke test: 100%|██████████| 4440/4440 [00:01<00:00, 2892.72it/s]"]},{"output_type":"stream","name":"stdout","text":["✅ Stratified split selesai.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"execution_count":null},{"cell_type":"markdown","source":"#### (2) Divide: train_test_split","metadata":{"id":"4b3eMIiEKKJk"}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom collections import Counter\n\n\n\n\n# 🔁 Transformasi\n# Augmentasi\ntrain_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n# Transformasi untuk validasi/test tanpa augmentasi acak\nval_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntargets = full_dataset.targets\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(targets)),      # Buat array dari index 0 sampai N-1\n    test_size=0.2,                # Alokasikan 20% untuk validasi\n    shuffle=True,\n    stratify=targets              # INI KUNCINYA: pastikan proporsi kelas sama\n)\n\ntrain_labels = [targets[i] for i in train_idx]\nprint(f\"Distribusi kelas di Training: {Counter(train_labels)}\")\nclass_counts = Counter(train_labels)\nnum_samples = len(train_labels)\nclass_weights = {\n    class_id: num_samples / count for class_id, count in class_counts.items()\n}\nweights = [class_weights[label] for label in train_labels]\nsampler = torch.utils.data.WeightedRandomSampler(\n    weights=torch.DoubleTensor(weights),\n    num_samples=num_samples\n)\n\nval_labels = [targets[i] for i in val_idx]\nprint(f\"Distribusi kelas di Validasi: {Counter(val_labels)}\")\n\ntrain_dataset = Subset(full_dataset, train_idx)\ntrain_dataset.dataset.transform = train_transform\n\nval_dataset = Subset(full_dataset, val_idx)\nval_dataset.dataset.transform = val_transform\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nprint(f\"Total data: {len(full_dataset)} gambar\")\nprint(f\"Data Training: {len(train_dataset)} gambar\")\nprint(f\"Data Validasi: {len(val_dataset)} gambar\")\nprint(f\"Nama Kelas: {class_names}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfbdY9yfKUgt","outputId":"d0f00232-38c0-4f60-e0d6-d4c533fd05c6","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T07:28:17.576781Z","iopub.execute_input":"2025-08-17T07:28:17.577491Z","iopub.status.idle":"2025-08-17T07:28:17.602315Z","shell.execute_reply.started":"2025-08-17T07:28:17.577466Z","shell.execute_reply":"2025-08-17T07:28:17.601531Z"}},"outputs":[{"name":"stdout","text":"Distribusi kelas di Training: Counter({4: 3233, 6: 3114, 5: 2611, 3: 2568, 1: 2487, 2: 2122, 0: 1206})\nDistribusi kelas di Validasi: Counter({4: 809, 6: 778, 5: 653, 3: 642, 1: 622, 2: 531, 0: 301})\nTotal data: 21677 gambar\nData Training: 17341 gambar\nData Validasi: 4336 gambar\nNama Kelas: ['background', 'glass', 'metal', 'organic', 'paper', 'plastic', 'textiles']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Data Distribution checking","metadata":{"id":"NZwJYq_hH4zT"}},{"cell_type":"markdown","source":"## Training and Evaluation","metadata":{"id":"atrzVjhIJPyq"}},{"cell_type":"markdown","source":"#### Setup ClearML\n\nGo ahead and sign-up/sign-in to [AI Infrastructure Platform | Maximize AI Performance & Scalability | ClearML](https://clear.ml/)\n\nAfter that, go to Settings -> Workspace -> Create new credentials\n\nThe new credentials will be created and shows two options:\n\nLocal Python (Recommended)\nJupyter Notebook\nBoth actually are the same things, it only differs on how to use the new credentials.\n\nThis time, use the clearml CLI app to consume the credentials, when prompted, paste it.","metadata":{"id":"s6PsY9duJkpc"}},{"cell_type":"code","source":"!pip install clearml","metadata":{"id":"1MObF1_cNqbO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e959fcb4-4c4c-4ab7-aab5-244e36c391d4","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:49:22.457416Z","iopub.execute_input":"2025-08-17T04:49:22.457812Z","iopub.status.idle":"2025-08-17T04:49:27.227291Z","shell.execute_reply.started":"2025-08-17T04:49:22.457793Z","shell.execute_reply":"2025-08-17T04:49:27.226495Z"}},"outputs":[{"name":"stdout","text":"Collecting clearml\n  Downloading clearml-2.0.2-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (25.3.0)\nCollecting furl>=2.0.0 (from clearml)\n  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (4.24.0)\nRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.26.4)\nCollecting pathlib2>=2.3.0 (from clearml)\n  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from clearml) (7.0.0)\nRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from clearml) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.9.0.post0)\nRequirement already satisfied: pyjwt<2.11.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.10.1)\nRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.11/dist-packages (from clearml) (6.0.2)\nRequirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.17.0)\nRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.5.0)\nRequirement already satisfied: Pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (11.2.1)\nRequirement already satisfied: referencing<0.40 in /usr/local/lib/python3.11/dist-packages (from clearml) (0.36.2)\nRequirement already satisfied: requests>=2.32.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.32.4)\nCollecting orderedmultidict>=1.0.1 (from furl>=2.0.0->clearml)\n  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (2025.4.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (0.25.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->clearml) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing<0.40->clearml) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.0->clearml) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.0->clearml) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.0->clearml) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10->clearml) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10->clearml) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10->clearml) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10->clearml) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10->clearml) (2024.2.0)\nDownloading clearml-2.0.2-py2.py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\nDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\nDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pathlib2, orderedmultidict, furl, clearml\nSuccessfully installed clearml-2.0.2 furl-2.1.4 orderedmultidict-1.0.1 pathlib2-2.3.7.post1\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"CLEARML_CONF = user_secrets.get_secret(\"CLEARML_CONF\")\n\n\nwith open(\"/root/clearml.conf\", \"w\") as f:\n    f.write(CLEARML_CONF)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:02:02.543762Z","iopub.execute_input":"2025-08-01T08:02:02.544414Z","iopub.status.idle":"2025-08-01T08:02:02.679335Z","shell.execute_reply.started":"2025-08-01T08:02:02.544386Z","shell.execute_reply":"2025-08-01T08:02:02.678790Z"},"id":"d1gbJSGZ1Z4b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!clearml-init","metadata":{"id":"TbVTxN62NtHk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42518faa-461f-4c2d-fd90-33b5746a0466","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:04:10.593939Z","iopub.execute_input":"2025-08-01T08:04:10.594221Z","iopub.status.idle":"2025-08-01T08:04:15.812038Z","shell.execute_reply.started":"2025-08-01T08:04:10.594200Z","shell.execute_reply":"2025-08-01T08:04:15.811266Z"}},"outputs":[{"output_type":"stream","name":"stdout","text":["ClearML SDK setup process\n","\n","Please create new clearml credentials through the settings page in your `clearml-server` web app (e.g. http://localhost:8080//settings/workspace-configuration) \n","Or create a free account at https://app.clear.ml/settings/workspace-configuration\n","\n","In settings page, press \"Create new credentials\", then press \"Copy to clipboard\".\n","\n","Paste copied configuration here:\n","api {   # Redi Ahmad Supriyatna's workspace   web_server: https://app.clear.ml/   api_server: https://api.clear.ml   files_server: https://files.clear.ml   credentials {     \"access_key\" = \"XP018Y5KX0LBYJ9JY0LSA1F8BRATNJ\"     \"secret_key\" = \"Tt_arUNOmgaL3_VIsCUmPAHXGYRpY70dKLnyh7Vln6Ew5eRUHdeeU8ODfiEHoRd0kT8\"   } }\n","Detected credentials key=\"XP018Y5KX0LBYJ9JY0LSA1F8BRATNJ\" secret=\"Tt_a***\"\n","\n","ClearML Hosts configuration:\n","Web App: https://app.clear.ml/\n","API: https://api.clear.ml\n","File Store: https://files.clear.ml\n","\n","Verifying credentials ...\n","Credentials verified!\n","\n","New configuration stored in /root/clearml.conf\n","ClearML setup completed successfully.\n"]}],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision matplotlib","metadata":{"id":"9Ry525mbBaZW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"21f5972d-0e61-4ed3-c39a-e5650c3c7ad9","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T04:49:37.389213Z","iopub.execute_input":"2025-08-17T04:49:37.389478Z","iopub.status.idle":"2025-08-17T04:50:45.481533Z","shell.execute_reply.started":"2025-08-17T04:49:37.389456Z","shell.execute_reply":"2025-08-17T04:50:45.480772Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#@title <b>Time Out Preventer (Advanced) </b></strong>\n%%capture\nAUTO_RECONNECT = True #@param {type:\"boolean\"}\n#@markdown **Run this code to prevent Google Colab from Timeout**\nfrom os import makedirs\nmakedirs(\"/root/.config/rclone\", exist_ok = True)\nif AUTO_RECONNECT:\n  import IPython\n  from google.colab import output\n\n  display(IPython.display.Javascript('''\n  function ClickConnect(){\n    btn = document.querySelector(\"colab-connect-button\")\n    if (btn != null){\n      console.log(\"Click colab-connect-button\");\n      btn.click()\n      }\n\n    btn = document.getElementById('ok')\n    if (btn != null){\n      console.log(\"Click reconnect\");\n      btn.click()\n      }\n    }\n\n  setInterval(ClickConnect,60000)\n  '''))","metadata":{"id":"1B-wiSQryw5C"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"while True:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:17.949923Z","iopub.execute_input":"2025-08-01T08:06:17.950217Z","iopub.status.idle":"2025-08-01T08:08:02.367528Z","shell.execute_reply.started":"2025-08-01T08:06:17.950194Z","shell.execute_reply":"2025-08-01T08:08:02.366287Z"},"id":"FAHo8dPY1Z4d","outputId":"95a1a037-4257-4ca9-9837-d2cae90c3199"},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2044646855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null},{"cell_type":"markdown","source":"### Training Options (Choose one of these)","metadata":{"id":"kVvKfsV9rx3r"}},{"cell_type":"markdown","source":"####  Choose Base Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ndef create_model(model_name: str, num_classes: int, feature_extract: bool = True):\n    \"\"\"\n    Memuat pre-trained model dari torchvision menggunakan pendekatan hardcode yang andal\n    dan menyesuaikannya untuk transfer learning. ✅\n\n    Args:\n        model_name (str): Nama model yang didukung (contoh: 'resnet50', 'mobilenet_v3_small').\n        num_classes (int): Jumlah kelas output untuk dataset baru.\n        feature_extract (bool): Jika True, bekukan bobot kecuali layer terakhir.\n                              Jika False, seluruh model akan dilatih (fine-tuning).\n\n    Returns:\n        torch.nn.Module: Model yang sudah disesuaikan dan siap pakai.\n    \"\"\"\n\n    # --- Langkah 1: Tentukan model yang didukung secara eksplisit ---\n    # Pendekatan ini lebih aman dan mencegah error akibat penamaan yang tidak konsisten.\n    supported_models = {\n        # ResNet Family\n        \"resnet18\": (models.resnet18, models.ResNet18_Weights.DEFAULT),\n        \"resnet50\": (models.resnet50, models.ResNet50_Weights.DEFAULT),\n        # MobileNet Family\n        \"mobilenet_v2\": (models.mobilenet_v2, models.MobileNet_V2_Weights.DEFAULT),\n        \"mobilenet_v3_small\": (models.mobilenet_v3_small, models.MobileNet_V3_Small_Weights.DEFAULT),\n        \"mobilenet_v3_large\": (models.mobilenet_v3_large, models.MobileNet_V3_Large_Weights.DEFAULT),\n        # EfficientNet Family\n        \"efficientnet_b0\": (models.efficientnet_b0, models.EfficientNet_B0_Weights.DEFAULT),\n        \"efficientnet_b7\": (models.efficientnet_b7, models.EfficientNet_B7_Weights.DEFAULT),\n        # Other Architectures\n        \"vgg16\": (models.vgg16, models.VGG16_Weights.DEFAULT),\n        \"densenet121\": (models.densenet121, models.DenseNet121_Weights.DEFAULT),\n        \"vit_b_16\": (models.vit_b_16, models.ViT_B_16_Weights.DEFAULT),\n        \"swin_t\": (models.swin_t, models.Swin_T_Weights.DEFAULT),\n    }\n\n    if model_name not in supported_models:\n        raise ValueError(\n            f\"Model '{model_name}' tidak didukung.\\n\"\n            f\"Model yang tersedia: {list(supported_models.keys())}\"\n        )\n\n    # Ambil constructor dan bobot dari dictionary\n    model_constructor, weights = supported_models[model_name]\n\n    # Buat instance model dengan bobot pre-trained\n    model = model_constructor(weights=weights)\n\n    # --- Langkah 2: Membekukan Bobot (jika feature_extract=True) ---\n    if feature_extract:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    # --- Langkah 3: Mengganti Layer Klasifikasi Terakhir ---\n    # Logika ini tetap sama karena sudah cukup robust\n    if hasattr(model, 'fc'): # Untuk ResNet, dll.\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n\n    elif hasattr(model, 'classifier'):\n        if isinstance(model.classifier, nn.Sequential): # Untuk VGG, MobileNet, EfficientNet\n            last_layer = model.classifier[-1]\n            if isinstance(last_layer, nn.Linear):\n                num_ftrs = last_layer.in_features\n                model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n            else:\n                raise TypeError(f\"Layer terakhir dari classifier ({type(last_layer)}) bukan nn.Linear.\")\n        elif isinstance(model.classifier, nn.Linear): # Untuk DenseNet\n            num_ftrs = model.classifier.in_features\n            model.classifier = nn.Linear(num_ftrs, num_classes)\n        else:\n            raise TypeError(f\"Tipe classifier ({type(model.classifier)}) tidak didukung.\")\n\n    elif hasattr(model, 'head'): # Untuk Vision Transformer, Swin Transformer\n        num_ftrs = model.head.in_features\n        model.head = nn.Linear(num_ftrs, num_classes)\n\n    else:\n        raise NameError(f\"Layer klasifikasi untuk '{model_name}' tidak ditemukan.\")\n\n    return (model_name, model)","metadata":{"id":"GnSRiWhuqaNx","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T06:14:32.343307Z","iopub.execute_input":"2025-08-17T06:14:32.343852Z","iopub.status.idle":"2025-08-17T06:14:32.352819Z","shell.execute_reply.started":"2025-08-17T06:14:32.343829Z","shell.execute_reply":"2025-08-17T06:14:32.352046Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_training_history(train_losses, val_losses, train_accs, val_accs):\n    \"\"\"\n    Fungsi untuk menampilkan grafik loss dan akurasi training & validation.\n    \"\"\"\n    # Buat 2 subplot berdampingan\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n    epochs_range = range(1, len(train_losses) + 1)\n\n    # Plot 1: Model Loss\n    ax1.plot(epochs_range, train_losses, 'o-', color='tab:blue', label='Training Loss')\n    ax1.plot(epochs_range, val_losses, 'o-', color='tab:orange', label='Validation Loss')\n    ax1.set_title('Model Loss', fontsize=16)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.legend(loc='upper right')\n    ax1.grid(True)\n\n    # Plot 2: Model Accuracy\n    ax2.plot(epochs_range, train_accs, 'o-', color='tab:green', label='Training Accuracy')\n    ax2.plot(epochs_range, val_accs, 'o-', color='tab:red', label='Validation Accuracy')\n    ax2.set_title('Model Accuracy', fontsize=16)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy', fontsize=12)\n    ax2.legend(loc='lower right')\n    ax2.grid(True)\n    \n    plt.suptitle('Visualisasi Metrik Pelatihan', fontsize=20, y=1.02)\n    plt.tight_layout()\n    \n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Fungsi untuk menampilkan confusion matrix.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                          xticklabels=class_names, yticklabels=class_names)\n    heatmap.set_xlabel('Kelas Prediksi', fontsize=12, labelpad=10)\n    heatmap.set_ylabel('Kelas Aktual', fontsize=12, labelpad=10)\n    heatmap.set_title('Confusion Matrix Final', fontsize=16, pad=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T06:43:54.024715Z","iopub.execute_input":"2025-08-17T06:43:54.025026Z","iopub.status.idle":"2025-08-17T06:43:54.032936Z","shell.execute_reply.started":"2025-08-17T06:43:54.025005Z","shell.execute_reply":"2025-08-17T06:43:54.032269Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Implementasi Focal Loss yang stabil secara numerik.\n    \n    Focal Loss = -alpha * (1 - pt) ** gamma * log(pt)\n    \n    di mana pt adalah probabilitas dari kelas yang benar.\n    \"\"\"\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        \"\"\"\n        Args:\n            alpha (Tensor, opsional): Tensor bobot untuk setiap kelas. \n                                     Bentuknya [num_classes].\n            gamma (float, opsional): Parameter pemfokusan. Default: 2.0.\n            reduction (str, opsional): 'mean', 'sum', atau 'none'. Default: 'mean'.\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs (Tensor): Prediksi logits dari model (sebelum softmax). \n                             Bentuknya [batch_size, num_classes].\n            targets (Tensor): Label kelas yang sebenarnya (ground truth). \n                              Bentuknya [batch_size].\n        \"\"\"\n        # Hitung log probabilitas (lebih stabil dari softmax + log)\n        log_pt = F.log_softmax(inputs, dim=1)\n        \n        # Dapatkan log probabilitas dari kelas yang benar\n        log_pt = log_pt.gather(1, targets.view(-1, 1))\n        log_pt = log_pt.view(-1)\n        \n        # Hitung probabilitas (pt) dari log probabilitas\n        pt = log_pt.exp()\n\n        # Jika alpha (bobot kelas) diberikan\n        if self.alpha is not None:\n            # Pastikan alpha berada di device yang sama dengan target\n            if self.alpha.type() != targets.data.type():\n                self.alpha = self.alpha.type_as(targets.data)\n            \n            # Pilih bobot alpha sesuai dengan target kelas\n            at = self.alpha.gather(0, targets.data)\n            \n            # Hitung cross entropy loss dengan bobot alpha\n            log_pt = log_pt * at\n\n        # Hitung loss utama\n        loss = -1 * (1 - pt) ** self.gamma * log_pt\n\n        # Terapkan reduksi (mean atau sum)\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T07:33:59.864300Z","iopub.execute_input":"2025-08-17T07:33:59.865048Z","iopub.status.idle":"2025-08-17T07:33:59.871927Z","shell.execute_reply.started":"2025-08-17T07:33:59.865023Z","shell.execute_reply":"2025-08-17T07:33:59.871154Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"#### (1) Training with early stopping - Recommended","metadata":{"id":"ILDzO2myN_Pg"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\nimport os\n\n\n# Impor ClearML hanya jika diperlukan\nif USE_CLEARML:\n    from clearml import Task, Logger\n\n# ⚙️ Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_name, model = create_model(model_name=\"efficientnet_b7\", num_classes=len(class_names))\nmodel = model.to(device)\n\n# Penanganan Dataset Tidak Seimbang\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(class_names),\n    y=class_names\n)\nweights = torch.tensor(class_weights, dtype=torch.float).to(device)\n# criterion = nn.CrossEntropyLoss(weight=weights)\ncriterion = FocalLoss(alpha=weights, gamma=2.0)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n\n# Kurangi LR saat val_loss tidak membaik\nscheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True)\n\n# 🔁 Pengaturan Training Loop\nepochs = 32\npatience = 5\ntrain_accs, val_accs = [], []\ntrain_losses, val_losses = [], []\nbest_val_acc = 0\nearly_stop_counter = 0\n\n# Penyimpanan Model\n# if USE_GOOGLE_COLAB:\n#     save_dir = \"/content/drive/MyDrive/AI_Models\"\n\nsave_dir = \"Models\"\nos.makedirs(save_dir, exist_ok=True)\nbest_model_path = os.path.join(save_dir, f\"{model_name}_best_model.pt\")\nlatest_model_path = os.path.join(save_dir, f\"{model_name}_latest_model.pt\")\n\n# Inisialisasi ClearML jika diaktifkan\nif USE_CLEARML:\n    task = Task.init(\n        project_name=\"EcoSort CNN\",\n        task_name=f\"{model_name} Training {time.strftime('%a, %b %-d, %Y - %H:%M:%S')}\",\n        task_type=Task.TaskTypes.training\n    )\n    logger = task.get_logger()\n\n# 🔍 Fungsi logging yang lebih fleksibel\ndef log_matplotlib_figure(fig, title, series, epoch):\n    if USE_CLEARML:\n        logger.report_matplotlib_figure(title=title, series=series, figure=fig, iteration=epoch)\n    plt.close(fig)\n\n# 🏃 Training dimulai\ntry:\n    for epoch in range(epochs):\n        start_time = time.time()\n        model.train()\n        train_loss, correct, total = 0, 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        train_acc = correct / total\n        train_loss_avg = train_loss / len(train_loader)\n        train_accs.append(train_acc.item()); train_losses.append(train_loss_avg)\n\n        if USE_CLEARML:\n            logger.report_scalar(\"Accuracy\", \"Train\", value=train_acc.item(), iteration=epoch)\n            logger.report_scalar(\"Loss\", \"Train\", value=train_loss_avg, iteration=epoch)\n            logger.report_scalar(\"LR\", \"Learning Rate\", value=optimizer.param_groups[0]['lr'], iteration=epoch)\n\n        # 🔍 Validasi\n        model.eval()\n        val_loss, correct, total = 0, 0, 0\n        y_true, y_pred = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, preds = torch.max(outputs, 1)\n                correct += torch.sum(preds == labels)\n                total += labels.size(0)\n                y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n        val_acc = correct / total\n        val_loss_avg = val_loss / len(val_loader)\n        val_accs.append(val_acc.item()); val_losses.append(val_loss_avg)\n        scheduler.step(val_loss_avg)\n\n        if USE_CLEARML:\n            logger.report_scalar(\"Accuracy\", \"Validation\", value=val_acc.item(), iteration=epoch)\n            logger.report_scalar(\"Loss\", \"Validation\", value=val_loss_avg, iteration=epoch)\n\n            # Classification Report & Metrics\n            report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n            for class_name, metrics in report.items():\n                if isinstance(metrics, dict): # Hindari 'accuracy' yang bukan dict\n                    logger.report_scalar(f\"F1-Score/{class_name}\", \"Validation\", value=metrics[\"f1-score\"], iteration=epoch)\n\n            # Confusion Matrix\n            cm = confusion_matrix(y_true, y_pred)\n            fig_cm, ax = plt.subplots(figsize=(8, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n            ax.set_title(\"Confusion Matrix\")\n            ax.set_xlabel(\"Predicted Label\")\n            ax.set_ylabel(\"True Label\")\n            log_matplotlib_figure(fig_cm, \"Confusion Matrix\", \"Validation\", epoch)\n\n        print(f\"🔁 Epoch {epoch+1}/{epochs}\")\n        print(f\"  → Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n        print(f\"  → Train Loss: {train_loss_avg:.4f} | Val Loss: {val_loss_avg:.4f}\")\n\n\n        # Simpan model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            early_stop_counter = 0\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"🏆 Model terbaik disimpan ke {best_model_path}\")\n        else:\n            early_stop_counter += 1\n            torch.save(model.state_dict(), latest_model_path)\n            print(f\"📦 Model terbaru disimpan ke {latest_model_path}\")\n            if early_stop_counter >= patience:\n                print(\"⏹️ Early stopping terpicu.\")\n                break\n\n        duration = time.time() - start_time\n        print(f\"⏱️ Waktu epoch: {duration:.2f} detik\")\n        if USE_CLEARML:\n            logger.report_scalar(\"Epoch Time (sec)\", \"Duration\", value=duration, iteration=epoch)\n\n        print()\n\nfinally:\n    # 🎉 Selesai\n    print(\"\\n=== Laporan Klasifikasi Akhir ===\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n    if USE_CLEARML and 'task' in locals():\n        print(\"Menutup task ClearML.\")\n        task.close()\n\n    if train_losses and val_losses and train_accs and val_accs:\n        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n        plot_confusion_matrix(y_true, y_pred, class_names)\n        plt.show()\n    else:\n        print(\"Tidak ada data histori training untuk diplot.\")","metadata":{"id":"nUEZ24awJXcT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e66e0e79-af49-4c9d-fd30-2beef996dae5","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T08:16:30.287064Z","iopub.execute_input":"2025-08-17T08:16:30.287694Z","iopub.status.idle":"2025-08-17T08:35:36.083354Z","shell.execute_reply.started":"2025-08-17T08:16:30.287672Z","shell.execute_reply":"2025-08-17T08:35:36.082091Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n100%|██████████| 255M/255M [00:01<00:00, 229MB/s] \n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🔁 Epoch 1/32\n  → Train Acc: 0.5949 | Val Acc: 0.6469\n  → Train Loss: 0.7370 | Val Loss: 0.6559\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 115.89 detik\n\n🔁 Epoch 2/32\n  → Train Acc: 0.6486 | Val Acc: 0.6515\n  → Train Loss: 0.5942 | Val Loss: 0.6080\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 116.56 detik\n\n🔁 Epoch 3/32\n  → Train Acc: 0.6571 | Val Acc: 0.6557\n  → Train Loss: 0.5764 | Val Loss: 0.5880\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 116.01 detik\n\n🔁 Epoch 4/32\n  → Train Acc: 0.6562 | Val Acc: 0.6619\n  → Train Loss: 0.5712 | Val Loss: 0.5727\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 116.18 detik\n\n🔁 Epoch 5/32\n  → Train Acc: 0.6553 | Val Acc: 0.6663\n  → Train Loss: 0.5627 | Val Loss: 0.5625\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 116.30 detik\n\n🔁 Epoch 6/32\n  → Train Acc: 0.6689 | Val Acc: 0.6672\n  → Train Loss: 0.5481 | Val Loss: 0.5554\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 116.16 detik\n\n🔁 Epoch 7/32\n  → Train Acc: 0.6598 | Val Acc: 0.6720\n  → Train Loss: 0.5641 | Val Loss: 0.5525\n🏆 Model terbaik disimpan ke Models/efficientnet_b7_best_model.pt\n⏱️ Waktu epoch: 115.70 detik\n\n🔁 Epoch 8/32\n  → Train Acc: 0.6618 | Val Acc: 0.6670\n  → Train Loss: 0.5581 | Val Loss: 0.5480\n📦 Model terbaru disimpan ke Models/efficientnet_b7_latest_model.pt\n⏱️ Waktu epoch: 116.22 detik\n\n🔁 Epoch 9/32\n  → Train Acc: 0.6531 | Val Acc: 0.6718\n  → Train Loss: 0.5713 | Val Loss: 0.5505\n📦 Model terbaru disimpan ke Models/efficientnet_b7_latest_model.pt\n⏱️ Waktu epoch: 115.74 detik\n\n\n=== Laporan Klasifikasi Akhir ===\n              precision    recall  f1-score   support\n\n  background       1.00      0.98      0.99        56\n       glass       0.67      0.64      0.65       168\n       metal       0.59      0.59      0.59       134\n     organic       0.72      0.76      0.74       160\n       paper       0.67      0.64      0.66       214\n     plastic       0.50      0.61      0.55       155\n    textiles       0.80      0.71      0.75       201\n\n    accuracy                           0.68      1088\n   macro avg       0.71      0.70      0.70      1088\nweighted avg       0.68      0.68      0.68      1088\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_125/1424567725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_125/1424567725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_accs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_125/2402818304.py\u001b[0m in \u001b[0;36mplot_training_history\u001b[0;34m(train_losses, val_losses, train_accs, val_accs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Plot 1: Model Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab:blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab:orange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (9,)"],"ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (10,) and (9,)","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x700 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"#### (2) Training without early stopping","metadata":{"id":"RRpj1OvSsAMc"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom collections import Counter\nimport time\n\n# Transform\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Dataset\ntrain_dataset = datasets.ImageFolder(\"dataset/train\", transform=transform)\nclass_names = train_dataset.classes\nprint(\"Label mapping:\", train_dataset.class_to_idx)\nprint(\"Distribusi:\", Counter([label for _, label in train_dataset]))\n\nval_dataset = datasets.ImageFolder(\"dataset/test\", transform=transform)\nprint(\"Label mapping:\", val_dataset.class_to_idx)\nprint(\"Distribusi:\", Counter([label for _, label in val_dataset]))\n\n# Split\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, len(class_names))\nmodel = model.to(device)\n\n# Loss & Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nepochs = 6\ntrain_accs, val_accs = [], []\nbest_val_acc = 0\n\nfor epoch in range(epochs):\n    model.train()\n    correct, total, loss_total = 0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        loss_total += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += torch.sum(preds == labels)\n        total += labels.size(0)\n\n    train_acc = correct / total\n    train_accs.append(train_acc.item())\n    print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f}\")\n\n    # Validasi\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n\n    val_acc = correct / total\n    val_accs.append(val_acc.item())\n    print(f\"            → Val Acc: {val_acc:.4f}\")\n\n# Simpan\ntorch.save(model.state_dict(), \"model_cnn.pt\")\nprint(\"✅ Model disimpan.\")","metadata":{"id":"z04jGaYgsFLW"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation","metadata":{"id":"OG66ezAGOKAn"}},{"cell_type":"code","source":"# 📊 Evaluasi dengan laporan & Confusion Matrix\nmodel.eval()\ny_true, y_pred = [], []\n\nfor images, labels in val_loader:\n    images = images.to(device)\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\n\n    y_true.extend(labels.numpy())\n    y_pred.extend(preds.cpu().numpy())\n\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# Plot akurasi training & val\nplt.plot(train_accs, label=\"Train\")\nplt.plot(val_accs, label=\"Validation\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.title(\"Train vs Validation Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"id":"9wqOURnbONd4"},"outputs":[],"execution_count":null}]}